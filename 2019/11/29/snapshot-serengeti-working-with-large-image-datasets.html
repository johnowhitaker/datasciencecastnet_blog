<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/datasciencecastnet_blog/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Snapshot Serengeti - Working with Large Image Datasets | datasciencecastnet</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Snapshot Serengeti - Working with Large Image Datasets" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Driven Data launched a competition around the Snapshot Serengeti database - something I’ve been intending to investigate for a while. Although the competition is called “Hakuna Ma-data” (which where I come from means something like “there is no data”), this is actually the largest dataset I’ve worked with to date, with ~5TB of high-res images. I suspect that that’s putting people off (there are only a few names on the leaderboard), so I’m writing this post to show how I did an entry, run through some tricks for dealing with big datasets, give you a notebook to get started quickly and try out a fun new tool I’ve found for monitoring long-running experiments using neptune.ml.Let’s dive in." />
<meta property="og:description" content="Driven Data launched a competition around the Snapshot Serengeti database - something I’ve been intending to investigate for a while. Although the competition is called “Hakuna Ma-data” (which where I come from means something like “there is no data”), this is actually the largest dataset I’ve worked with to date, with ~5TB of high-res images. I suspect that that’s putting people off (there are only a few names on the leaderboard), so I’m writing this post to show how I did an entry, run through some tricks for dealing with big datasets, give you a notebook to get started quickly and try out a fun new tool I’ve found for monitoring long-running experiments using neptune.ml.Let’s dive in." />
<link rel="canonical" href="https://johnowhitaker.github.io/datasciencecastnet_blog/2019/11/29/snapshot-serengeti-working-with-large-image-datasets.html" />
<meta property="og:url" content="https://johnowhitaker.github.io/datasciencecastnet_blog/2019/11/29/snapshot-serengeti-working-with-large-image-datasets.html" />
<meta property="og:site_name" content="datasciencecastnet" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-11-29T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Driven Data launched a competition around the Snapshot Serengeti database - something I’ve been intending to investigate for a while. Although the competition is called “Hakuna Ma-data” (which where I come from means something like “there is no data”), this is actually the largest dataset I’ve worked with to date, with ~5TB of high-res images. I suspect that that’s putting people off (there are only a few names on the leaderboard), so I’m writing this post to show how I did an entry, run through some tricks for dealing with big datasets, give you a notebook to get started quickly and try out a fun new tool I’ve found for monitoring long-running experiments using neptune.ml.Let’s dive in.","@type":"BlogPosting","headline":"Snapshot Serengeti - Working with Large Image Datasets","dateModified":"2019-11-29T00:00:00-06:00","datePublished":"2019-11-29T00:00:00-06:00","url":"https://johnowhitaker.github.io/datasciencecastnet_blog/2019/11/29/snapshot-serengeti-working-with-large-image-datasets.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://johnowhitaker.github.io/datasciencecastnet_blog/2019/11/29/snapshot-serengeti-working-with-large-image-datasets.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/datasciencecastnet_blog/assets/main.css">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://johnowhitaker.github.io/datasciencecastnet_blog/feed.xml" title="datasciencecastnet" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/datasciencecastnet_blog/">datasciencecastnet</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/datasciencecastnet_blog/about/">About Me</a><a class="page-link" href="/datasciencecastnet_blog/search/">Search</a><a class="page-link" href="/datasciencecastnet_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Snapshot Serengeti - Working with Large Image Datasets</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-11-29T00:00:00-06:00" itemprop="datePublished">
        Nov 29, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Driven Data launched a <a href="https://www.drivendata.org/competitions/59/camera-trap-serengeti/leaderboard/">competition around the Snapshot Serengeti database</a> - something I’ve been intending to investigate for a while. Although the competition is called “Hakuna Ma-data” (which where I come from means something like “there is no data”), this is actually the largest dataset I’ve worked with to date, with ~5TB of high-res images. I suspect that that’s putting people off (there are only a few names on the leaderboard), so I’m writing this post to show how I did an entry, run through some tricks for dealing with big datasets, give you a notebook to get started quickly and try out a fun new tool I’ve found for monitoring long-running experiments using <a href="http://neptune.ml/">neptune.ml</a>.Let’s dive in.</p>

<h2 id="the-challenge">The Challenge</h2>

<p>The goal of the competition is to create a model that can correctly label the animal(s) in an image sequence from one of many camera traps scattered around the Serengeti plains, which are teeming with wildlife. You can read more about the data and the history of the project on their <a href="https://www.zooniverse.org/projects/zooniverse/snapshot-serengeti">website</a>. There can be more than one type of animal in an image, making this a multi-label classification problem.</p>

<p><img src="/datasciencecastnet_blog/images/wordpress_export/2019/11/screenshot-from-2019-11-29-09-59-08.png?w=797" alt="" /></p>

<p>Some not-so-clear images from the dataset</p>

<p>The drivendata competition is interesting in that you aren’t submitting predictions. Instead, you have to submit everything needed to perform inference in their hidden test environment. In other words, you have to submit a trained model and the code to make it go. This is a good way to practice model deployment.</p>

<h2 id="modelling">Modelling</h2>

<p>The approach I took to modelling is very similar to the other fastai projects I’ve done recently. Get a pre-trained resnet50 model, tune the head, unfreeze, fine-tune, and optionally re-train with larger images right at the end. It’s a multi-label classification problem, so I followed the fastai planet labs example for labeling the data. You can see the details of the code in the notebook (coming in the next section) but I’m not going to go over it all again here. The modelling in this case is less interesting than the extra things needed to work at this scale.</p>

<h2 id="starter-notebook">Starter Notebook</h2>

<p>I’m a big fan of making data science and ML more accessible. For anyone intimidated by the scale of this contest, and not too keen on following the path I took in the rest of this post, I’ve created a <a href="https://colab.research.google.com/drive/1pOjQXXCCa6fTzw4w5V3DI8ey28ul_9yz">Google Colab Notebook to get you started</a>. It shows how to get some of the data, label it, create and train a model, score your model like they do in the competition and create a submission. This should help you get started, and will give a good score without modification. The notebook also has some obvious improvements waiting to be made - using more data, training the model further…..</p>

<p><img src="/datasciencecastnet_blog/images/wordpress_export/2019/11/screenshot-from-2019-11-29-10-45-06.png?w=696" alt="" /></p>

<p>Training a quick model in the starter notebook</p>

<p>The code in the notebook is essentially what I used for my first submission, which is currently the top out of the… 2 total submissions on the leaderboard. As much as I like looking good, I’ll be much happier if this helps a bunch of people jump ahead of that score! Please let me know if you use this, so that I don’t feel that this wasn’t useful to anyone?</p>

<h2 id="moar-data---colab-wont-cut-it">Moar Data - Colab won’t cut it</h2>

<p>OK, so there definitely isn’t 5TB of storage on Google Colab, and even though we can get a decent score with a fraction of the data, what if we want to go further? My approach was as follows:</p>

<ul>
  <li>Create a Google Cloud Compute instance with all the fastai libraries etc installed, by following <a href="https://course.fast.ai/start_gcp.html">this tutorial</a>. The resultant machine has 50GB memory, a P100 GPU and 200GB disk space by default. It comes with most of what’s required for deep learning work, and has the added bonus of having jupyter + all the fastai course notebooks ready to get things going quickly. I made sure not to make the instance preemptible - we want to have long-running tasks going, so having it shut down unexpectedly would be sad.</li>
  <li>Add an extra disk to the compute instance. <a href="https://devopscube.com/mount-extra-disks-on-google-cloud/">This tutorial</a> gave me the main steps. It was quite surreal typing in 6000 GB for the size! I mounted the dist at <code class="language-plaintext highlighter-rouge">/ss_ims</code> - that will be my base folder going forward.</li>
  <li>Download a season of data, and then begin experimenting while more downloads. No point having that pricey GPU sitting idle!</li>
  <li>Train the full model overnight, tracking progress.</li>
  <li>Submit!</li>
</ul>

<p><img src="/datasciencecastnet_blog/images/wordpress_export/2019/11/screenshot-from-2019-11-22-18-01-13.png?w=1024" alt="" /></p>

<p>Mounting a scarily large disk!</p>

<p>I won’t go into the cloud setup here, but in the next section let’s look at how you can track the status of a long-running experiment.</p>

<h2 id="neptune-ml---tracking-progress">Neptune ML - Tracking progress</h2>

<p>I’d set the experiments running on my cloud machine, but due to lack of electricity and occasional loss of connection I couldn’t simply leave my laptop running and connected to the VM to show how the model training was progressing. With so many images, each epoch of training took ages, and I had a couple of models crash early in the process. This was frustrating - I would try to leave it going overnight but if the model failed in the evening it meant that I had wasted some of my few remaining cloud credits on a machine sitting idle. Luckily, I had recently seen how to monitor progress remotely, meaning I could check my phone while I was out and see if the model was working and how good it was getting.</p>

<p><img src="/datasciencecastnet_blog/images/wordpress_export/2019/11/screenshot-from-2019-11-25-09-19-18.png?w=1024" alt="" /></p>

<p>Tracking loss and metrics over time with neptune.ml</p>

<p>The process is pretty simple, and well documented <a href="https://medium.com/neptune-ml/track-and-organize-fastai-experimentation-process-in-neptune-78ec8d6b18b0">here</a>. You sign up for an account, get an API key and add a callback to your model. This will then let you log in to neptune.ml from any device, and track your loss, any metrics you’ve added and the output of the code you’re running. I could give more reasons why this is useful, but honestly the main motivation is that it’s cool! I had great fun surreptitiously checking my loss from my phone every half hour while I was out and about.</p>

<p><img src="/datasciencecastnet_blog/images/wordpress_export/2019/11/screenshot-from-2019-11-29-11-54-25.png?w=796" alt="" /></p>

<p>Tracking model training with neptune</p>

<h2 id="where-next">Where next?</h2>

<p>I’m out of cloud credits, and as an ‘independent scientist’ my funding situation doesn’t really justify spending more money on cloud compute to try a better entry. If you’d like to sponsor some more work, I may have another go with a properly trained model. I did manage to experiment on using more than the first image in a sequence, and using Jeremy Howard’s trick of doing some final fine-tuning on larger images - would be interesting to see how much these improve the score in this contest.</p>

<p>I hope this post encourages more of you to try this contest out! As the starter notebook shows, you can get close to the top (beating the benchmark) with a tiny fraction of the data and some simple tricks. <a href="https://colab.research.google.com/drive/1pOjQXXCCa6fTzw4w5V3DI8ey28ul_9yz">Give it a try</a> and report how you do in the comments!</p>

  </div><a class="u-url" href="/datasciencecastnet_blog/2019/11/29/snapshot-serengeti-working-with-large-image-datasets.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/datasciencecastnet_blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">datasciencecastnet</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">datasciencecastnet</li><li><a class="u-email" href="mailto:johnowhitaker@yahoo.com">johnowhitaker@yahoo.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/johnowhitaker"><svg class="social svg-icon"><use xlink:href="/datasciencecastnet_blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">johnowhitaker</span></a></li><li><a href="https://www.twitter.com/johnowhitaker"><svg class="social svg-icon"><use xlink:href="/datasciencecastnet_blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">johnowhitaker</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Sharing my finds as I trawl through the world of data science</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
