<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/datasciencecastnet_blog/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Attempting Zindi’s Farm Pin Crop Detection Challenge Without Downloading any Imagery | datasciencecastnet</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Attempting Zindi’s Farm Pin Crop Detection Challenge Without Downloading any Imagery" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Zindi is currently hosting a competition to classify fields by crop type using Sentinel-2 satellite imagery. They provide labeled fields with the crop type, and a separate set of fields as the ‘test’ set. The goal is to use the provided imagery to predict the crop type as accurately as possible. It’s a great contest, BUT: The imagery files are huge (although they offer Azure credits to help mitigate this by using cloud computing), and extending such an analysis to other areas is not easy. The goal of this post is to show how we can use the labeled fields to train our own classifier in Google Earth Engine (GEE), using Landsat imagery for the classification (EDIT: Sentinel 2 imagery is also available in GEE, making this choice somewhat arbitrary). This results in a model that can be applied over any region, and is a process that could be replicated by anyone with some known crop fields and an internet connection." />
<meta property="og:description" content="Zindi is currently hosting a competition to classify fields by crop type using Sentinel-2 satellite imagery. They provide labeled fields with the crop type, and a separate set of fields as the ‘test’ set. The goal is to use the provided imagery to predict the crop type as accurately as possible. It’s a great contest, BUT: The imagery files are huge (although they offer Azure credits to help mitigate this by using cloud computing), and extending such an analysis to other areas is not easy. The goal of this post is to show how we can use the labeled fields to train our own classifier in Google Earth Engine (GEE), using Landsat imagery for the classification (EDIT: Sentinel 2 imagery is also available in GEE, making this choice somewhat arbitrary). This results in a model that can be applied over any region, and is a process that could be replicated by anyone with some known crop fields and an internet connection." />
<link rel="canonical" href="https://johnowhitaker.github.io/datasciencecastnet_blog/2019/06/26/tutorial-predicting-crop-types-with-gee.html" />
<meta property="og:url" content="https://johnowhitaker.github.io/datasciencecastnet_blog/2019/06/26/tutorial-predicting-crop-types-with-gee.html" />
<meta property="og:site_name" content="datasciencecastnet" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-26T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Zindi is currently hosting a competition to classify fields by crop type using Sentinel-2 satellite imagery. They provide labeled fields with the crop type, and a separate set of fields as the ‘test’ set. The goal is to use the provided imagery to predict the crop type as accurately as possible. It’s a great contest, BUT: The imagery files are huge (although they offer Azure credits to help mitigate this by using cloud computing), and extending such an analysis to other areas is not easy. The goal of this post is to show how we can use the labeled fields to train our own classifier in Google Earth Engine (GEE), using Landsat imagery for the classification (EDIT: Sentinel 2 imagery is also available in GEE, making this choice somewhat arbitrary). This results in a model that can be applied over any region, and is a process that could be replicated by anyone with some known crop fields and an internet connection.","@type":"BlogPosting","headline":"Attempting Zindi’s Farm Pin Crop Detection Challenge Without Downloading any Imagery","dateModified":"2019-06-26T00:00:00-05:00","datePublished":"2019-06-26T00:00:00-05:00","url":"https://johnowhitaker.github.io/datasciencecastnet_blog/2019/06/26/tutorial-predicting-crop-types-with-gee.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://johnowhitaker.github.io/datasciencecastnet_blog/2019/06/26/tutorial-predicting-crop-types-with-gee.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/datasciencecastnet_blog/assets/main.css">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://johnowhitaker.github.io/datasciencecastnet_blog/feed.xml" title="datasciencecastnet" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/datasciencecastnet_blog/">datasciencecastnet</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/datasciencecastnet_blog/about/">About Me</a><a class="page-link" href="/datasciencecastnet_blog/search/">Search</a><a class="page-link" href="/datasciencecastnet_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Attempting Zindi’s Farm Pin Crop Detection Challenge Without Downloading any Imagery</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-06-26T00:00:00-05:00" itemprop="datePublished">
        Jun 26, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><a href="http://zindi.africa">Zindi</a> is currently hosting a <a href="https://zindi.africa/competitions/farm-pin-crop-detection-challenge">competition</a> to classify fields by crop type using Sentinel-2 satellite imagery. They provide labeled fields with the crop type, and a separate set of fields as the ‘test’ set. The goal is to use the provided imagery to predict the crop type as accurately as possible. It’s a great contest, BUT: The imagery files are huge (although they offer Azure credits to help mitigate this by using cloud computing), and extending such an analysis to other areas is not easy. The goal of this post is to show how we can use the labeled fields to train our own classifier in Google Earth Engine (GEE), using Landsat imagery for the classification (EDIT: Sentinel 2 imagery is also available in GEE, making this choice somewhat arbitrary). This results in a model that can be applied over any region, and is a process that could be replicated by anyone with some known crop fields and an internet connection.</p>

<p>I won’t include all the code here. Instead, view it and try it for yourself <a href="https://code.earthengine.google.com/afa4cec9b21f3835f3a5e1feb39beb5d">here</a>.</p>

<h2 id="the-training-data">The Training Data</h2>

<p>The important data is contained in a shapefile (a mapping-related file format for ‘vector’ layers that can contain points, lines or polygons). It contains multiple features (polygons), each representing a field with a certain kind of crop. The crop type is encoded as a number from 1 to 10. More info <a href="https://zindi.africa/competitions/farm-pin-crop-detection-challenge/data">here</a>.</p>

<p><img src="images/wordpress_export/2019/06/screenshot-from-2019-06-25-18-44-51.png" alt="" /></p>

<p>Some features in the ‘train’ shapefile.</p>

<p>We can upload this data as an asset in GEE by using the ‘New Table Upload’ option and selecting all the files except <code class="language-plaintext highlighter-rouge">train.qpj</code> (which is unnecessary). I named the asset ‘farm_zindi_train’, and repeated the steps for the test dataset.</p>

<p><img src="images/wordpress_export/2019/06/screenshot-from-2019-06-25-18-10-58.png" alt="" /></p>

<p>There is one last hurdle we must overcome when using this data to train classifiers in GEE. Each feature in the training shapefile contains a property, ‘Crop_Id_Ne’, that tells us the crop type. Unfortunately, this is represented as a string. To convert it to the required type, we create a function that is mapped over the feature collection and use <code class="language-plaintext highlighter-rouge">ee.Number.parse()</code> to convert the string into a number for the model to use.</p>

<p><img src="images/wordpress_export/2019/06/screenshot-from-2019-06-26-10-09-21.png" alt="" /></p>

<p>Getting the required properties in the correct type by mapping a function over the collection</p>

<h2 id="landsat-imagery">Landsat Imagery</h2>

<p>Instead of the Sentinel-2 imagery the competition is using, we’ll see if we can achieve the same results with freely available Landsat 8 imagery. I used code from <a href="https://developers.google.com/earth-engine/tutorial_api_06">this tutorial</a> to load the landsat data and create a ‘greenest pixel composite’ based on a computed value called NDVI (normalized difference vegetation index). This is not an ideal approach - we could instead have chosen a time of year when the differences between crops are most obvious, or used multiple images from different times in the growing season. These improvements will be considered in a future tutorial.</p>

<h2 id="training-a-classifier">Training A Classifier</h2>

<p>The <a href="https://developers.google.com/earth-engine/classification">‘Supervised Classification’ guide</a> by Google is good place to start when attempting this kind of classification task. The only changes I made to the provided code was to change the references to match my own training data, tweak the scale to reduce memory use and specify the property we’re trying to predict (in our case, ‘CID’ for crop ID). Looking at the output, it seems to roughly match the farm outlines - a good sign.</p>

<p><img src="images/wordpress_export/2019/06/screenshot-from-2019-06-26-10-29-47.png" alt="" /></p>

<p>Classifier output with farm boundaries shown.</p>

<h2 id="comparing-classification-accuracy">Comparing Classification Accuracy</h2>

<p>Ideally, we’d split the data into training and test sets, compare different classifiers and pick the best. We might even keep a third set of data, the ‘validation’ set, to get a better idea of how our chosen classifier will perform on unseen data. As with the different options for input layers, I’ll leave this for a second tutorial. For now, we will be lazy and evaluate the accuracy on the training data: print(‘Accuuracy’, trained.confusionMatrix().accuracy());</p>

<p>The accuracy of a CART classifier is listed as <strong>65%</strong>. Not bad, given that there are 10 classes, but not great either. Switching to a random forest model gives a much higher accuracy score, but may be subject to overfitting.</p>

<h2 id="exporting-predictions">Exporting Predictions</h2>

<p>To get the predicted crop type in each region of the test file, we look at the most common crop type predicted by the classifier in each region and export the predictions to a CSV file:</p>

<p><img src="images/wordpress_export/2019/06/screenshot-from-2019-06-26-10-37-39.png" alt="" /></p>

<p>Exporting predictions</p>

<p>This results in a file containing columns for Field_Id and predicted crop type. Normally, this is what we’d like. However, the Zindi contest specifies the submission with predicted probabilities for each different crop:</p>

<p><img src="images/wordpress_export/2019/06/screenshot-from-2019-06-26-10-40-17.png" alt="" /></p>

<p>The submission format</p>

<p>To get the data in this format, I used Python and pandas, with the pandas get_dummies function:</p>

<p><img src="images/wordpress_export/2019/06/screenshot-from-2019-06-26-10-42-31.png" alt="" /></p>

<p>Formatting the data correctly</p>

<p>This is not ideal - we see a 1 for our predicted class, with 0s for the rest. It would be better to predict the probabilities and hedge our bets, but let’s see see how this does. <code class="language-plaintext highlighter-rouge">predictions.to_csv('pred_test_cart.csv', index=False)</code> gives a file we can upload on Zindi… And the final score? ~17.4 (or ~15 with the random forest model), putting this submission in 30th place out of 31 entries as of today.</p>

<h2 id="future-improvements">Future Improvements</h2>

<p>There are many ways we could improve this score. A different classifier might perform better. Selecting the greenest pixels was probably not the best approach. Instead of using ee.Reducer.mode(), we could count how many pixels are predicted for each crop type and use those counts to assign probabilities for our submission. Etc Etc. Some of these improvements will be covered in a future tutorial, hopefully coming soon.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Despite our lackluster score, this exercise has hopefully shown the possibilities of this approach. Using only freely available imagery, which we never had to download thanks to Google Earth Engine, we were able to make predictions about which crops were being grown in different fields. If you’ve followed along, I hope you’ve seen what is possible with GEE - simply by copying snippets of code and gluing them all together. Once the accuracy is improved, this technique could be applied in many different situations.</p>

  </div><a class="u-url" href="/datasciencecastnet_blog/2019/06/26/tutorial-predicting-crop-types-with-gee.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/datasciencecastnet_blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">datasciencecastnet</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">datasciencecastnet</li><li><a class="u-email" href="mailto:johnowhitaker@yahoo.com">johnowhitaker@yahoo.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/johnowhitaker"><svg class="social svg-icon"><use xlink:href="/datasciencecastnet_blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">johnowhitaker</span></a></li><li><a href="https://www.twitter.com/johnowhitaker"><svg class="social svg-icon"><use xlink:href="/datasciencecastnet_blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">johnowhitaker</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Sharing my finds as I trawl through the world of data science</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
